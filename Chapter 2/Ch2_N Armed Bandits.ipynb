{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning <em> in Action </em>\n",
    "## N-Armed Bandits\n",
    "### Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the main contextual bandit class we'll be using as our environment/simulator to train a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBandit:\n",
    "    def __init__(self, arms=10, nstates=10):\n",
    "        self.arms = arms\n",
    "        self.nstates = nstates\n",
    "        self.init_distribution()\n",
    "        self.update_state()\n",
    "        self.reward_scale = 10\n",
    "        \n",
    "    def init_distribution(self):\n",
    "        # each row represents a state, each column an arm\n",
    "        self.bandit_matrix = np.random.rand(self.nstates, self.arms)\n",
    "\n",
    "    def max_expected_reward(self):\n",
    "        max_probs_per_state = np.max(self.bandit_matrix, axis=1)\n",
    "        expected_payoff_per_state = self.reward_scale * max_probs_per_state\n",
    "        return np.mean(expected_payoff_per_state), expected_payoff_per_state\n",
    "    \n",
    "    def reward(self, prob):\n",
    "        reward = 0\n",
    "        for i in range(self.reward_scale):\n",
    "            if random.random() < prob:\n",
    "                reward += 1\n",
    "        return reward\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def update_state(self):\n",
    "        self.state = np.random.randint(0, self.nstates)\n",
    "        \n",
    "    def get_reward(self,arm):\n",
    "        return self.reward(self.bandit_matrix[self.get_state()][arm])\n",
    "        \n",
    "    def choose_arm(self, arm):\n",
    "        reward = self.get_reward(arm)\n",
    "        self.update_state()\n",
    "        return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our simple neural network model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(av, tau=1.12):\n",
    "    n = len(av)\n",
    "    probs = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        softm = ( np.exp(av[i] / tau) / np.sum( np.exp(av[:] / tau) ) )\n",
    "        probs[i] = softm\n",
    "    return probs\n",
    "\n",
    "def one_hot(N, pos, val=1):\n",
    "    one_hot_vec = np.zeros(N)\n",
    "    one_hot_vec[pos] = val\n",
    "    return one_hot_vec\n",
    "\n",
    "arms = 10\n",
    "nstates = 2\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 1, arms, 100, arms\n",
    "\n",
    "model = th.nn.Sequential(\n",
    "    th.nn.Linear(D_in, H),\n",
    "    th.nn.ReLU(),\n",
    "    th.nn.Linear(H, D_out),\n",
    "    th.nn.ReLU(),\n",
    ")\n",
    "\n",
    "loss_fn = th.nn.MSELoss(size_average=False)\n",
    "\n",
    "env = ContextBandit(arms, nstates)\n",
    "\n",
    "mean_r, dis_r = env.max_expected_reward()\n",
    "print(mean_r, dis_r)\n",
    "print(env.bandit_matrix)\n",
    "for i in range(env.bandit_matrix.shape[0]):\n",
    "    print(np.argmax(env.bandit_matrix[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the training function, which accepts an instantiated ContextBandit object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env):\n",
    "    epochs = 5000\n",
    "    #one-hot encode current state\n",
    "    cur_state = Variable(th.Tensor(one_hot(arms,env.get_state())))\n",
    "    counts = np.zeros((nstates, arms))\n",
    "    reward_hist = np.zeros(50)\n",
    "    reward_hist[:] = 5\n",
    "    runningMean = np.average(reward_hist)\n",
    "    learning_rate = 1e-2\n",
    "    optimizer = th.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    plt.xlabel(\"Plays\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    for i in tqdm(range(epochs)):\n",
    "        y_pred = model(cur_state) #produce reward predictions\n",
    "        av_softmax = softmax(y_pred.data.numpy(), tau=2.0) #turn reward distribution into probability distribution\n",
    "        av_softmax /= av_softmax.sum() #make sure total prob adds to 1\n",
    "        choice = np.random.choice(arms, p=av_softmax) #sample an action\n",
    "        counts[int(th.argmax(cur_state).numpy()), choice] += 1\n",
    "        cur_reward = env.choose_arm(choice)\n",
    "        one_hot_reward = y_pred.data.numpy().copy()\n",
    "        one_hot_reward[choice] = cur_reward\n",
    "        reward = Variable(th.Tensor(one_hot_reward))\n",
    "        loss = loss_fn(y_pred, reward)\n",
    "        if i % 50 == 0:\n",
    "            runningMean = np.average(reward_hist)\n",
    "            reward_hist[:] = 0\n",
    "            plt.scatter(i, runningMean)\n",
    "        reward_hist[i % 50] = cur_reward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "        cur_state = Variable(th.Tensor(one_hot(arms,env.get_state())))\n",
    "        \n",
    "    print(counts)\n",
    "    for i in range(nstates):\n",
    "        print(np.argmax(counts[i]))\n",
    "#    print(act_rewards)\n",
    "    print(av_softmax) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
